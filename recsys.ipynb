{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prefinal_recsys.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uA2Pablwo8u-",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import string\n",
        "import random\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import movie_reviews, stopwords,wordnet\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import sys\n",
        "\n",
        "from rake_nltk import Rake\n",
        "\n",
        "import numpy as np\n",
        "from numpy import arange,atleast_2d,argsort\n",
        "\n",
        "from collections import defaultdict,Counter\n",
        "from surprise import Prediction\n",
        "from surprise import Reader, Dataset, SVD, accuracy,KNNBaseline,NMF\n",
        "from surprise.model_selection import train_test_split,cross_validate\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from ast import literal_eval\n",
        "\n",
        "import operator\n",
        "from functools import reduce\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKnq4pH9ueE5",
        "colab_type": "code",
        "outputId": "6e712738-59af-4ebd-bc73-f886441720e4",
        "colab": {}
      },
      "source": [
        "from pandarallel import pandarallel\n",
        "pandarallel.initialize()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: Pandarallel will run on 80 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-nkOEi7aF_kL",
        "colab": {}
      },
      "source": [
        "helpfulCutOffRatings = 20\n",
        "reviewDataPath = 'amazon/vsmall.csv'\n",
        "train_data_location = '/freespace/local/lp642/Final_20_Movies_and_TV_train.tsv.gz'\n",
        "test_data_location = '/freespace/local/lp642/Final_20_Movies_and_TV_test.tsv.gz'\n",
        "metadataPath = '/freespace/local/lp642/meta_Movies_and_TV.json'\n",
        "topN = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2EOyHX3dF_kN",
        "colab": {}
      },
      "source": [
        "def removeUnusedReviewInfo(rdata):\n",
        "    rdata = rdata.drop(['unixReviewTime','reviewTime','reviewerName'],axis=1,errors='ignore')\n",
        "    return rdata\n",
        "    \n",
        "def createReviewerInfo(data):\n",
        "    reviewerInfo = data[['reviewerID', 'reviewerName']]\n",
        "    return reviewerInfo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LF9Hs7kRF_kT",
        "colab": {}
      },
      "source": [
        "def createUserMovieMatrix(udata,mdata):\n",
        "    fullData = udata.merge(mdata, on='asin',how='outer')\n",
        "    userMovieMatrix = fullData.pivot_table(index='reviewerID', columns = 'asin',values = 'overall',dropna=False).reset_index().rename_axis(None, axis=1)\n",
        "    userMovieMatrix.set_index('reviewerID',inplace=True)\n",
        "    userMovieMatching = pd.crosstab(index = fullData['reviewerID'], columns=fullData['asin'],dropna = False)\n",
        "    userMovieMatrix = userMovieMatrix.replace(np.nan,0)\n",
        "#     userMovieMatrix = userMovieMatrix.drop(['reviewerID'],axis = 1)\n",
        "    return userMovieMatrix, userMovieMatching\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hBqr28czF_kW",
        "colab": {}
      },
      "source": [
        "# Classify data into negative or positive\n",
        "def classifyBaseReviewRating(x):\n",
        "    if x > 2.5:\n",
        "        return \"positive\"\n",
        "    if x == 2.5:\n",
        "        return \"neutral\"\n",
        "    if x < 2.5:\n",
        "        return \"negative\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JVRDs0DuF_kZ",
        "colab": {}
      },
      "source": [
        "def cleanData(r):\n",
        "    r = str(r)\n",
        "    if len(r) == 0:\n",
        "        return ''\n",
        "    if type(r) == float:\n",
        "        return ''\n",
        "    return r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YpOvmxxMF_kb",
        "colab": {}
      },
      "source": [
        "def getWordnetType(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def cleanReview(r):\n",
        "    r = str(r)\n",
        "    if len(r) == 0:\n",
        "        return ''\n",
        "    if type(r) == float:\n",
        "        return ''\n",
        "    r = r.lower()\n",
        "    #remove puncutation\n",
        "    r = [eachWord.strip(string.punctuation) for eachWord in r.split(\" \") ]\n",
        "    #remove numbers\n",
        "    r = [eachWord for eachWord in r if not any(l.isdigit() for l in eachWord)]\n",
        "    \n",
        "    #remove stop words\n",
        "    stop = stopwords.words('english')\n",
        "    r = [w for w in r if w not in stop]\n",
        "    \n",
        "    #remove words which are empty\n",
        "    r = [w for w in r if len(w) >0]\n",
        "    \n",
        "    #add pos_tags\n",
        "    pos_tags = pos_tag(r)\n",
        "    \n",
        "    #word lemmatize\n",
        "    r = [WordNetLemmatizer().lemmatize(t[0], getWordnetType(t[1])) for t in pos_tags]\n",
        "    \n",
        "    #remove useless words\n",
        "    r = [w for w in r if len(w) > 1]\n",
        "    \n",
        "    #recreate review\n",
        "    r = \" \".join(r)\n",
        "    return r\n",
        "\n",
        "# get compound score from VADER and append it as review rating\n",
        "def getSentimentScore(rdata):\n",
        "    sa = SentimentIntensityAnalyzer()\n",
        "    rdata[\"sentiments\"] = rdata[\"reviewText\"].parallel_apply(lambda x: sa.polarity_scores(x))\n",
        "    rdata = pd.concat([rdata.drop(['sentiments'], axis=1), rdata['sentiments'].parallel_apply(pd.Series)], axis=1)\n",
        "    # Not using compound score because of data limitation. Only few movies have compound score availble on amazon data.\n",
        "    # rdata['helpful'] = rdata['helpful'].parallel_apply(lambda x: getHelpfulScore(x)) \n",
        "    \n",
        "    #compound score from VADER is from -1 to +1. Scaling it to 0 to 5 and multiplying it with helpfulness score\n",
        "    rdata['reviewRating'] = (1+(rdata['compound']+1)*2)\n",
        "    # rdata['reviewRating'] = (1+(rdata['compound']+1)*2) * rdata['helpful']\n",
        "    rdata = rdata.drop(['compound','pos','neg','neu'],axis = 1,errors='ignore')\n",
        "    rdata = rdata.drop(['reviewText','helpful'],axis=1,errors='ignore')\n",
        "    return rdata\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aGz7z2XXF_kj",
        "colab": {}
      },
      "source": [
        "def getHelpfulScore(x):\n",
        "    if x == None:\n",
        "        return 1.0\n",
        "    x = literal_eval(x)\n",
        "    a = float(x[0])\n",
        "    b = float(x[1])\n",
        "\n",
        "    if b != 0 and a + b > helpfulCutOffRatings-1:\n",
        "        return a / (a + b)\n",
        "    else:\n",
        "        return 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QkR1tAD_F_km",
        "colab": {}
      },
      "source": [
        "def generateSentimentScoreOnReviews(rdata):\n",
        "    reviewerInfo = createReviewerInfo(rdata)\n",
        "    rdata = removeUnusedReviewInfo(rdata)\n",
        "    rdata[\"reviewType\"] = rdata[\"overall\"].parallel_apply(classifyBaseReviewRating)\n",
        "    rdata['reviewText'] = rdata['reviewText'] + rdata['summary']\n",
        "    rdata = rdata.drop(['summary'],axis = 1,errors='ignore')\n",
        "    rdata['reviewText'] = rdata['reviewText'].parallel_apply(cleanData)    \n",
        "    rdata['reviewText'] = rdata['reviewText'].parallel_apply(lambda x:cleanReview(x))\n",
        "    rdata = getSentimentScore(rdata)\n",
        "    return rdata\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ks4JgDiFF_ko",
        "colab": {}
      },
      "source": [
        "def getMovieData(path):\n",
        "    return pd.read_json(path,lines=True,typ='frame')\n",
        "\n",
        "def preProcessMovieData(moviedata):\n",
        "    moviedata.sort_values(\"asin\",inplace=True)\n",
        "    moviedata.drop_duplicates(subset='asin',keep=False,inplace = True)\n",
        "    moviedata = moviedata.reset_index(drop=True)\n",
        "    moviedata = moviedata.drop(['rank','main_cat','image','also_buy','also_view','price','details','feature','date','tech1'],axis = 1,errors='ignore')\n",
        "    return moviedata\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Su1gGv_YF_kq",
        "colab": {}
      },
      "source": [
        "def getCosineSimilarity(moviedata):\n",
        "    count = CountVectorizer()\n",
        "    count_matrix = count.fit_transform(moviedata['cleanText'])\n",
        "    # generating the cosine similarity matrix\n",
        "    cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
        "    return cosine_sim\n",
        "\n",
        "def getMovieSimilarityFromPlot(moviedata):\n",
        "    # initializing the new column\n",
        "    moviedata['impWords'] = ''\n",
        "    moviedata['cleanText'] = ''\n",
        "    for index, movie in moviedata.iterrows():\n",
        "        tot = ''\n",
        "        description = movie['description']\n",
        "        if description != None:\n",
        "            if type(description) != float:\n",
        "                if len(description) != 0:\n",
        "                    for i in description:\n",
        "                        if type(i) != str:\n",
        "                            print(description)\n",
        "                        tot = tot + i\n",
        "\n",
        "        #remove \"Movies & TV\", \"Genre for Featured Categories\", \"Movies\", \"Independently Distributed\", \"All Titles\", \"All\", \"4-for-3 DVD\", \n",
        "        ignoreWords = [\"Movies & TV\", \"Genre for Featured Categories\", \"Movies\", \"Independently Distributed\", \"All Titles\", \"All\", \"4-for-3 DVD\"]\n",
        "        category = movie['category']\n",
        "        if category != None:\n",
        "            if type(category) != float:\n",
        "                for w in category:\n",
        "                    if w not in ignoreWords:\n",
        "                        tot = tot+w\n",
        "        brand = movie['brand']\n",
        "        if brand != None:\n",
        "            if type(brand) != float:\n",
        "                tot = tot + i\n",
        "        r = Rake()\n",
        "        r.extract_keywords_from_text(tot)\n",
        "        # getting the dictionary whith key words as keys and their scores as values\n",
        "        key_words_dict_scores = r.get_word_degrees()\n",
        "\n",
        "        # assigning the key words to the new column for the corresponding movie\n",
        "        movie['impWords'] = list(key_words_dict_scores.keys())\n",
        "        movie['cleanText'] = \" \".join(map(str,movie['impWords']))\n",
        "    moviedata = moviedata.drop(['category','brand','description','impWords'],axis = 1,errors='ignore')\n",
        "    cosine_sim =  getCosineSimilarity(moviedata)\n",
        "    moviedata = moviedata.drop(['cleanText'],axis = 1)\n",
        "    return cosine_sim, moviedata\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "poHqJ2YVF_ku",
        "colab": {}
      },
      "source": [
        "def getMovieAvgRating(moviedata,rdataf):\n",
        "    movieAvgRating = rdataf.groupby('asin')['overall'].agg(['mean', 'median', 'size'])\n",
        "    movieAvgRating.columns = ['rating_mean', 'rating_median', 'num_ratings']\n",
        "    return movieAvgRating\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4AOAbX-XF_kx",
        "colab": {}
      },
      "source": [
        "def get_top_recommendations(predictions, topN = 3):\n",
        "     \n",
        "    top_recs = defaultdict(list)\n",
        "    for uid, iid, true_r, est, _ in predictions:\n",
        "        top_recs[uid].append((iid, est))\n",
        "     \n",
        "    for uid, user_ratings in top_recs.items():\n",
        "        user_ratings.sort(key = lambda x: x[1], reverse = True)\n",
        "        top_recs[uid] = user_ratings[:topN]\n",
        "     \n",
        "    return top_recs\n",
        "\n",
        "def precision_recall_at_k(predictions, k=10, threshold=3.5, flag=True):\n",
        "    '''Return precision and recall at k metrics for each user.'''\n",
        "\n",
        "    # First map the predictions to each user.\n",
        "    user_est_true = defaultdict(list)\n",
        "    if flag == False:\n",
        "        for uid, _,true_r, est in predictions:\n",
        "            user_est_true[uid].append((est, true_r))\n",
        "    else:      \n",
        "        for uid, _, true_r, est, _ in predictions:\n",
        "            user_est_true[uid].append((est, true_r))\n",
        "            \n",
        "    precisions = dict()\n",
        "    recalls = dict()\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "\n",
        "        # Sort user ratings by estimated value\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # Number of relevant items\n",
        "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
        "\n",
        "        # Number of recommended items in top k\n",
        "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
        "\n",
        "        # Number of relevant and recommended items in top k\n",
        "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
        "                              for (est, true_r) in user_ratings[:k])\n",
        "\n",
        "        # Precision@K: Proportion of recommended items that are relevant\n",
        "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
        "\n",
        "        # Recall@K: Proportion of relevant items that are recommended\n",
        "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
        "\n",
        "    return precisions, recalls\n",
        "\n",
        "\n",
        "def getNDCGScore(tuples):\n",
        "    userPredictedRatings = defaultdict(list)\n",
        "    userActualRatings = defaultdict(list)\n",
        "    for ele in tuples:\n",
        "        userPredictedRatings[ele[0]].append(ele[3])\n",
        "        userActualRatings[ele[0]].append(ele[2])\n",
        "    for predictedRatings,actualRatings in zip(userPredictedRatings.items(),userActualRatings.items()):\n",
        "        predictedRatings[1].sort( reverse = True) \n",
        "        actualRatings[1].sort(reverse = True) \n",
        "    \n",
        "    mean_ndcg = 0\n",
        "\n",
        "    for predictedRatings,actualRatings in zip(userPredictedRatings.items(),userActualRatings.items()):\n",
        "        dcg = 0\n",
        "        idcg = 0\n",
        "        count = 0\n",
        "        for predictedRating,actualRating in zip(predictedRatings[1],actualRatings[1]):\n",
        "            count = count + 1\n",
        "            dcg = dcg + (predictedRating/math.log(count+1,2))\n",
        "            idcg = idcg + (actualRating/math.log(count+1,2))\n",
        "        mean_ndcg = mean_ndcg + (dcg/idcg)\n",
        "    return (mean_ndcg/len(userPredictedRatings))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TcbEKZj-F_k1",
        "colab": {}
      },
      "source": [
        "def getPredictionFromMovieContent(moviedata, rdata, testSplit):\n",
        "    moviedata = preProcessMovieData(moviedata)\n",
        "    # craete cosine similarity matrix\n",
        "    cosine_sim, moviedata = getMovieSimilarityFromPlot(moviedata)\n",
        "    train = rdata.sample(frac = testSplit,random_state=101)\n",
        "    testdata = rdata.drop(train.index)\n",
        "    rdata = train\n",
        "    del train\n",
        "    rdata = rdata.merge(moviedata, on='asin',how='left')\n",
        "    movieAvgRating = getMovieAvgRating(moviedata,rdata)\n",
        "    movieAvgRating.columns = ['rating_mean', 'rating_median', 'num_ratings']\n",
        "    moviedata = moviedata.merge(movieAvgRating, on='asin',how='left')\n",
        "    \n",
        "    predictions = getUserRatingPredictionFromMovieContent(cosine_sim, rdata, testdata, moviedata)\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EPeSXd6LF_k3",
        "colab": {}
      },
      "source": [
        "def getUserRatingPredictionFromMovieContent(cosine_sim, rdata, testdata, moviedata):\n",
        "    \n",
        "    # Get user movie prediction based on train data\n",
        "    userMovieMatrix, userMovieMatching = createUserMovieMatrix(rdata, moviedata)\n",
        "    sumUserMovieWeight = np.dot(userMovieMatrix, cosine_sim)\n",
        "    sumSimFoRM2M = np.dot(userMovieMatching, cosine_sim)\n",
        "    userMoviePredict = sumUserMovieWeight/sumSimFoRM2M #Movie prediction for all the data\n",
        "    \n",
        "    #Get User movie matching for test data\n",
        "    userMovieMatrixTest, userMovieMatchingTest = createUserMovieMatrix(testdata, moviedata)\n",
        "    \n",
        "    # Used for predicting not rated movies in train data\n",
        "    userMovieMatching = 1 - userMovieMatching\n",
        "    \n",
        "    # get user movie matrix for users not rated all\n",
        "    userMovieMatchingNewPred = userMovieMatching - userMovieMatchingTest\n",
        "    \n",
        "    # Predict all user-movie from train data\n",
        "    userMoviePredictMissingMovies = userMoviePredict * userMovieMatching\n",
        "    \n",
        "    # Prediction for already rated movies\n",
        "    userMovieAlreadyRatedPredicted = userMovieMatchingTest * userMoviePredictMissingMovies\n",
        "    \n",
        "    #Prediction for new movies\n",
        "    userMovieNewPredictions = userMoviePredictMissingMovies * userMovieMatchingNewPred\n",
        "    \n",
        "    \n",
        "    userMovieNewPredictions.reset_index(in_place=True)\n",
        "    userMovieNewPredictions = userMovieNewPredictions.replace(0,np.nan)\n",
        "    \n",
        "    userMovieAlreadyRatedPredicted.reset_index(in_place=True)\n",
        "    userMovieAlreadyRatedPredicted = userMovieAlreadyRatedPredicted.replace(0,np.nan)\n",
        "    \n",
        "    userMovieAlreadyRatedPredicted = pd.melt(userMovieAlreadyRatedPredicted, id_vars='reviewerID', \n",
        "                                        value_vars=list(userMovieAlreadyRatedPredicted.columns[1:]),\n",
        "                                        var_name='asin', \n",
        "                                        value_name='rating')\n",
        "    userMovieAlreadyRatedPredicted = userMovieAlreadyRatedPredicted[userMovieAlreadyRatedPredicted['rating'].notna()]\n",
        "    testdataF = testdata.merge(metadata, on='asin',how='outer')\n",
        "    \n",
        "    testdataF = testdataF[testdataF['overall'].notna()]\n",
        "    \n",
        "    userMovieAlreadyRatedPredictedFinal = pd.merge(userMovieAlreadyRatedPredicted, \n",
        "                                                   testdataF, how='left',on=['reviewerID','asin'])\n",
        "    # Can drop if not needed. Few movies does not have meta data in amazon data. So removing it.\n",
        "    # userMovieAlreadyRatedPredictedFinal.drop(['title'],axis =1,inplace=True)\n",
        "    \n",
        "    userMovieNewPredictions = pd.melt(userMovieNewPredictions, id_vars='reviewerID', \n",
        "                                        value_vars=list(userMovieNewPredictions.columns[1:]),\n",
        "                                        var_name='asin', \n",
        "                                        value_name='rating')\n",
        "    userMovieNewPredictions = userMovieNewPredictions[userMovieNewPredictions['rating'].notna()]\n",
        "    \n",
        "    combinedPred = pd.concat([userMovieAlreadyRatedPredictedFinal, userMovieNewPredictions],ignore_index=True)\n",
        "    \n",
        "    combinedPred = combinedPred.replace(np.nan,0)\n",
        "    combinedPred['tup'] = tuple(zip(combinedPred.reviewerID, combinedPred.asin,\n",
        "                                    combinedPred.rating,combinedPred.overall))\n",
        "    predictions = list(combinedPred['tup'])\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hEsLEAmYF_k6",
        "colab": {}
      },
      "source": [
        "def partition_input_data(input_location):\n",
        "    import pandas as pd\n",
        "    from collections import Counter\n",
        "    df_chunk = pd.read_csv(input_location,skiprows=1, sep='\\t',chunksize=1000000)\n",
        "    reviewers1 = Counter()\n",
        "    reviewers2 = Counter()\n",
        "\n",
        "    train_data = []\n",
        "    test_data = []\n",
        "    dfObj = pd.DataFrame()\n",
        "\n",
        "    for chunk in df_chunk:\n",
        "        for row in chunk.iterrows():\n",
        "            for ele in row[1].iteritems():\n",
        "                if ele[0] == 'reviewerID':\n",
        "                    reviewers1[ele[1]] = reviewers1[ele[1]] + 1\n",
        "                    reviewers2[ele[1]] = reviewers2[ele[1]] + 1\t\n",
        "\n",
        "\n",
        "    df_chunk = []\n",
        "    df_chunk = pd.read_csv(input_location,skiprows=1, sep='\\t',chunksize=1000000)\n",
        "\n",
        "    for chunk in df_chunk:\n",
        "        for row in chunk.iterrows():\n",
        "            new_row = []\n",
        "            for ele in row[1].iteritems():\n",
        "                new_row.append(ele[1])\n",
        "                if ele[0] == 'reviewerID':\n",
        "                    if 0.2 * reviewers1[ele[1]] >= reviewers2[ele[1]]:\n",
        "                        test_data.append(new_row)\n",
        "                    else:\n",
        "                        train_data.append(new_row)\n",
        "                    reviewers2[ele[1]] = reviewers2[ele[1]] - 1\n",
        "\n",
        "    \n",
        "    train_frame = pd.DataFrame(train_data,columns =['overall', 'verified', 'reviewTime','reviewerID','asin','style','reviewerName','reviewText'\n",
        "    ,'summary','unixReviewTime','vote','image'])\n",
        "    reader = Reader(rating_scale=(0.5, 5.0))\n",
        "    train_data = Dataset.load_from_df(train_frame[['reviewerID', 'asin', 'overall']], reader)\n",
        "    '''\n",
        "    train_frame.to_csv(train_data_location, sep='\\t',\n",
        "                          compression='infer')\n",
        "    '''\n",
        "    test_frame = pd.DataFrame(test_data,columns =['overall', 'verified', 'reviewTime','reviewerID','asin','style','reviewerName','reviewText'\n",
        "    ,'summary','unixReviewTime','vote','image'])\n",
        "    test_data = Dataset.load_from_df(test_frame[['reviewerID', 'asin', 'overall']], reader)\n",
        "    print('testframe',test_frame)\n",
        "    print('testdata',test_data)\n",
        "    \n",
        "    '''\n",
        "    test_frame.to_csv(test_data_location, sep='\\t',\n",
        "                          compression='infer')\n",
        "    '''\n",
        "    return (train_data, test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XDXESJuFF_k9",
        "colab": {}
      },
      "source": [
        "# Based on field in and type\n",
        "def getSurpriseRatingPrediction(train_data, ratingCol, test_data, algo, userBased):\n",
        "    \n",
        "    reader = Reader(rating_scale=(0.5, 5.0))\n",
        "    data = pd.concat([train_data,test_data])\n",
        "\n",
        "    traindata = Dataset.load_from_df(train_data[['reviewerID', 'asin', ratingCol]], reader)\n",
        "\n",
        "    testdata = Dataset.load_from_df(test_data[['reviewerID', 'asin', ratingCol]], reader)\n",
        "\n",
        "    dataf = Dataset.load_from_df(data[['reviewerID', 'asin', ratingCol]], reader)\n",
        "\n",
        "\n",
        "    raw_trainset = [traindata.raw_ratings[i] for i in range(len(traindata.raw_ratings))]\n",
        "    raw_testset = [testdata.raw_ratings[i] for i in range(len(testdata.raw_ratings))]\n",
        "\n",
        "    trainset = dataf.construct_trainset(raw_trainset)\n",
        "    testset = dataf.construct_testset(raw_testset)\n",
        "    \n",
        "    algoName = algo\n",
        "    if algo == 'svd':\n",
        "        algo = SVD()\n",
        "    elif algo == 'knn':\n",
        "        sim_options = {'name': 'cosine',\n",
        "               'user_based': userBased\n",
        "               }\n",
        "        algo = algo = KNNBaseline(sim_options=sim_options)\n",
        "    elif algo == 'NMF':\n",
        "        algo = NMF()\n",
        "    algo.fit(trainset)\n",
        "    print('mean for ',algoName, ' is ',trainset.global_mean)\n",
        "    predictions = algo.test(testset)\n",
        "    precisions, recalls =  precision_recall_at_k(predictions, k=10, threshold=3.5)\n",
        "    rmse = accuracy.rmse(predictions, verbose=True)\n",
        "    mae = accuracy.mae(predictions, verbose=True)\n",
        "    p = sum(prec for prec in precisions.values()) / len(precisions)\n",
        "    r = sum(prec for prec in recalls.values()) / len(recalls)\n",
        "    f = 2*p*r/ (p+r)\n",
        "    print('precisions for ',algoName, ' is ', p)\n",
        "    print('recall for ',algoName, ' is ', r)\n",
        "    print('F measure for ',algoName, ' is ', f)\n",
        "    print('MAE measure for ',algoName, ' is ', mae)\n",
        "    \n",
        "    return predictions\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "93PbkZ6GF_lB",
        "colab": {}
      },
      "source": [
        "def getPredictions(rdata, moviedata, testSplit, topN):\n",
        "    #get NMF predictions\n",
        "    NMFPredictions = getSurpriseRatingPrediction(rdata, 'overall', testSplit, 'NMF', False)\n",
        "    \n",
        "    # get SVD predictions\n",
        "    svdPredictions = getSurpriseRatingPrediction(rdata, 'overall', testSplit, 'svd', False )\n",
        "    \n",
        "    #get Knn user based predictions based on ratings\n",
        "    userBasedKNNPredictions = getSurpriseRatingPrediction(rdata, 'overall', testSplit, 'knn', True)\n",
        "    \n",
        "    #get Knn movie based predictions based on ratings\n",
        "    ratingBasedKNNPredictions = getSurpriseRatingPrediction(rdata, 'overall', testSplit, 'knn', False)\n",
        "    \n",
        "    #get Knn movie based predictions based on reviews\n",
        "    rdata = generateSentimentScoreOnReviews(rdata)\n",
        "    testSplit = generateSentimentScoreOnReviews(testSplit)\n",
        "\n",
        "    # Can use review ratings as actual ratings and predict the ratings. This can be used in case users have not provided ratings\n",
        "    #reviewBasedKNNPredictions = getSurpriseRatingPrediction(rdata, 'reviewRating', testSplit, 'knn', False)\n",
        "\n",
        "    reviewBasedKNNPredictions = list(testSplit.parallel_apply(lambda x: \n",
        "                                  Prediction(x.reviewerID,x.asin, x.overall, x.reviewRating, {}), axis = 1))\n",
        "    \n",
        "    #get content based predictions\n",
        "    contentBasedPredictions = getPredictionFromMovieContent(moviedata, rdata, testSplit)\n",
        "    print('content', contentBasedPredictions)\n",
        "    topNRec = getFinalRec(svdPredictions,\n",
        "                          userBasedKNNPredictions,\n",
        "                          ratingBasedKNNPredictions,\n",
        "                          reviewBasedKNNPredictions,\n",
        "                          NMFPredictions, contentBasedPredictions\n",
        "                          topN)\n",
        "    return topNRec\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rV81fQf2F_lF",
        "colab": {}
      },
      "source": [
        "def mergeDict(dicts):\n",
        "    mergeDict = defaultdict(list)\n",
        "    for dictEach in dicts:\n",
        "        for key, val in dictEach.items(): \n",
        "            mergeDict[key].append(val)\n",
        "    \n",
        "    newMergeDic = defaultdict(list)\n",
        "    for user, movieList in mergeDict.items():\n",
        "        movieDict = defaultdict(list)\n",
        "        movieList = reduce(operator.concat,movieList)\n",
        "        for movie, true_r, rating in movieList:\n",
        "            movieDict[movie].append((true_r,rating))\n",
        "        movieL = movieDict[movie]\n",
        "        newMergeDic[user] = dict(movieDict)\n",
        "\n",
        "    finalUserList = []\n",
        "    for user, movieList in newMergeDic.items():\n",
        "        for movie, ratingList in movieList.items():\n",
        "            avgRating = sum(x[1] for x in ratingList)/float(len(ratingList))\n",
        "            finalUserList.append((user,movie,ratingList[0][0],avgRating))\n",
        "    \n",
        "    detail = {}\n",
        "    predictions = [Prediction(uid,\n",
        "                                iid,\n",
        "                                r_ui_trans,\n",
        "                                est,\n",
        "                                detail)\n",
        "                       for (uid, iid, r_ui_trans,est) in finalUserList]\n",
        "    return predictions, finalUserList"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kktCD3WdueGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getPrecessionRecall(predictions, algoName, flag =True, str1='',str2=''):\n",
        "    precisions, recalls =  precision_recall_at_k(predictions, k=5, threshold=3.5,flag = flag)\n",
        "    rmse = accuracy.rmse(predictions, verbose=True)\n",
        "    p = sum(prec for prec in precisions.values()) / len(precisions)\n",
        "    r = sum(prec for prec in recalls.values()) / len(recalls)\n",
        "    f = 2*p*r/ (p+r)\n",
        "    mae = accuracy.mae(predictions, verbose=True)\n",
        "    if flag==False:\n",
        "        print('precisions for ',algoName, ' is ', p)\n",
        "        print('recall for ',algoName, ' is ', r)\n",
        "        print('F measure for ',algoName, ' is ', f)\n",
        "    else:\n",
        "        print('precisions for ',algoName, ' is ', p,'for ',str1,' ',str2)\n",
        "        print('recall for ',algoName, ' is ', r,'for ',str1,' ',str2)\n",
        "        print('F measure for ',algoName, ' is ', f,'for ',str1,' ',str2)\n",
        "        \n",
        "    return rmse, p, r, f ,mae\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mEcZIVRMF_lH",
        "colab": {}
      },
      "source": [
        "# Combine all the predictions score by weighted average\n",
        "def combinePredictions(svdPredictions, userBasedKNNPredictions,\n",
        "                                           ratingBasedKNNPredictions, reviewBasedKNNPredictions, NMFPredictions,\n",
        "                                          ):\n",
        "    svdW = 1\n",
        "    rknnW = 1\n",
        "    uknnW = 1\n",
        "    reKnnW = 1\n",
        "    svdPredictionsNew = defaultdict(list)\n",
        "    for uid, iid, true_r, est, _ in svdPredictions:\n",
        "            svdPredictionsNew[uid].append((iid,true_r, svdW*est))\n",
        "    \n",
        "    userBasedKNNPredictionsNew = defaultdict(list)\n",
        "    for uid, iid, true_r, est, _ in userBasedKNNPredictions:\n",
        "            userBasedKNNPredictionsNew[uid].append((iid,true_r, uknnW*est))\n",
        "            \n",
        "    ratingBasedKNNPredictionsNew = defaultdict(list)\n",
        "    for uid, iid, true_r, est, _ in ratingBasedKNNPredictions:\n",
        "            ratingBasedKNNPredictionsNew[uid].append((iid,true_r, rknnW*est))\n",
        "    \n",
        "    reviewBasedKNNPredictionsNew = defaultdict(list)\n",
        "    for uid, iid, true_r, est, _ in reviewBasedKNNPredictions:\n",
        "            reviewBasedKNNPredictionsNew[uid].append((iid,true_r, reKnnW*est))\n",
        "    \n",
        "    nmfPredictionsNew = defaultdict(list)\n",
        "    for uid, iid, true_r, est, _ in NMFPredictions:\n",
        "            nmfPredictionsNew[uid].append((iid,true_r, svdW*est))\n",
        "            \n",
        "    contentBasedPredictionsNew = defaultdict(list)\n",
        "    for uid, iid, true_r, est in contentBasedPredictions:\n",
        "           contentBasedPredictionsNew[uid].append((iid, est))\n",
        "    \n",
        "    combinedPred = [svdPredictionsNew, userBasedKNNPredictionsNew, \n",
        "                    ratingBasedKNNPredictionsNew,nmfPredictionsNew]\n",
        "                    \n",
        "    ratingPredictions, ratingPredictionsList = mergeDict(combinedPred)\n",
        "    ratingPredictionsListNew = defaultdict(list)\n",
        "    for uid, iid, true_r, est in ratingPredictionsList:\n",
        "            ratingPredictionsListNew[uid].append((iid,true_r, est))\n",
        "    \n",
        "    getPrecessionRecall(ratingPredictions, flag=True, algoName='before adding reviewRating')\n",
        "    print('NDCG for CF', getNDCGScore(ratingPredictionsList))\n",
        "\n",
        "    \n",
        "    combinedPred = [ratingPredictionsListNew, reviewBasedKNNPredictionsNew]\n",
        "    finalPredictions, finalPredictionsL = mergeDict(combinedPred)\n",
        "    getPrecessionRecall(finalPredictions, flag=True, algoName='after adding reviewRating')\n",
        "    print('NDCG for CF+SA', getNDCGScore(finalPredictionsL))\n",
        "    return finalPredictions\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qIJmJGTIF_lJ",
        "colab": {}
      },
      "source": [
        "def getFinalRec(svdPredictions, userBasedKNNPredictions,\n",
        "                ratingBasedKNNPredictions, reviewBasedKNNPredictions,\n",
        "                NMFPredictions, contentBasedPredictions\n",
        "                                          topN):\n",
        "    \n",
        "    predictions = combinePredictions(svdPredictions, \n",
        "                                    userBasedKNNPredictions,\n",
        "                                    ratingBasedKNNPredictions, \n",
        "                                    reviewBasedKNNPredictions,\n",
        "                                    NMFPredictions, \n",
        "                                    contentBasedPredictions\n",
        "                                        )\n",
        "\n",
        "    top_recs = defaultdict(list)\n",
        "    for uid, iid, true_r, est, _ in predictions:\n",
        "        top_recs[uid].append((iid, est))\n",
        "     \n",
        "    for uid, user_ratings in top_recs.items():\n",
        "        user_ratings.sort(key = lambda x: x[1], reverse = True)\n",
        "        top_recs[uid] = user_ratings[:topN]\n",
        "     \n",
        "    return top_recs\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gNF6cToWF_lL",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r_0nyf22F_lO",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMPDOQ43ueGr",
        "colab_type": "code",
        "outputId": "7dc3e7b5-a821-4497-fa30-2a9c7d8c1f85",
        "colab": {}
      },
      "source": [
        "\n",
        "train_data = pd.read_csv(train_data_location,  header=0, sep='\\t', quotechar='\"',  names=[ 'overall','verified','reviewTime','reviewerID','asin', 'style', 'reviewerName','reviewText','summary','unixReviewTime','vote','image'], skiprows=1)\n",
        "train_data = train_data.drop(['verified','vote','image','unixReviewTime','style','reviewTime'],axis =1,errors='ignore')\n",
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/ilab/users/lp642/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>asin</th>\n",
              "      <th>reviewerName</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>A3P98J5DZ00A75</td>\n",
              "      <td>0005019281</td>\n",
              "      <td>Ken Roberts</td>\n",
              "      <td>Henry Winkler proves his acting ability in thi...</td>\n",
              "      <td>grey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>A2U7DG83EXUSFP</td>\n",
              "      <td>0005019281</td>\n",
              "      <td>Donald R. Brandeberry</td>\n",
              "      <td>A good movie with morals</td>\n",
              "      <td>Family movie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>A1XQVED2NX33TN</td>\n",
              "      <td>0005019281</td>\n",
              "      <td>In my opinion...</td>\n",
              "      <td>More of a 'modern' version of the classic.  I ...</td>\n",
              "      <td>Was happily surprised...great version, esp for...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>ARSGS4RQUVL1O</td>\n",
              "      <td>0005019281</td>\n",
              "      <td>M. Smith</td>\n",
              "      <td>The Christmas Carol is my all time favorite st...</td>\n",
              "      <td>The Christmas Carol is my all time favorite st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>A101IGU6UDKW3X</td>\n",
              "      <td>0005019281</td>\n",
              "      <td>DorothyZ</td>\n",
              "      <td>Well made Christmas movie. It's a little slow ...</td>\n",
              "      <td>It's a little slow but sweet natured.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   overall      reviewerID        asin           reviewerName  \\\n",
              "1        4  A3P98J5DZ00A75  0005019281            Ken Roberts   \n",
              "2        5  A2U7DG83EXUSFP  0005019281  Donald R. Brandeberry   \n",
              "3        5  A1XQVED2NX33TN  0005019281       In my opinion...   \n",
              "4        5   ARSGS4RQUVL1O  0005019281               M. Smith   \n",
              "5        3  A101IGU6UDKW3X  0005019281               DorothyZ   \n",
              "\n",
              "                                          reviewText  \\\n",
              "1  Henry Winkler proves his acting ability in thi...   \n",
              "2                           A good movie with morals   \n",
              "3  More of a 'modern' version of the classic.  I ...   \n",
              "4  The Christmas Carol is my all time favorite st...   \n",
              "5  Well made Christmas movie. It's a little slow ...   \n",
              "\n",
              "                                             summary  \n",
              "1                                               grey  \n",
              "2                                       Family movie  \n",
              "3  Was happily surprised...great version, esp for...  \n",
              "4  The Christmas Carol is my all time favorite st...  \n",
              "5              It's a little slow but sweet natured.  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkcr1vUfueGy",
        "colab_type": "code",
        "outputId": "e9617860-7500-4f83-9ed4-3ced25d27d6d",
        "colab": {}
      },
      "source": [
        "\n",
        "test_data = pd.read_csv(test_data_location,  header=0, sep='\\t', quotechar='\"',  names=[ 'overall','verified','reviewTime','reviewerID','asin', 'style', 'reviewerName','reviewText','summary','unixReviewTime','vote','image'], skiprows=1)\n",
        "test_data = test_data.drop(['verified','vote','image','unixReviewTime','style','reviewTime'],axis =1,errors='ignore')\n",
        "test_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/ilab/users/lp642/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>asin</th>\n",
              "      <th>reviewerName</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>A16BJ43Z46QS3N</td>\n",
              "      <td>6304077955</td>\n",
              "      <td>C. L Wilson</td>\n",
              "      <td>I didn't really look forward to seeing this mo...</td>\n",
              "      <td>no title</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>A3AS6G2F9CDHV5</td>\n",
              "      <td>6304154178</td>\n",
              "      <td>Brian May</td>\n",
              "      <td>This adventure, penned no less by Douglas Adam...</td>\n",
              "      <td>\"They slammed him to the wall with good vibrat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>A2JS2OU5SBH1XQ</td>\n",
              "      <td>6304233639</td>\n",
              "      <td>Studebaker Hoch, billythemtn@geocities.com</td>\n",
              "      <td>One can only guess at what the producers of th...</td>\n",
              "      <td>Just what were they after?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>A2JS2OU5SBH1XQ</td>\n",
              "      <td>6304258984</td>\n",
              "      <td>Studebaker Hoch, billythemtn@geocities.com</td>\n",
              "      <td>Sure, the plot is so full of holes that you co...</td>\n",
              "      <td>One Helluva Ride!!!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>A2JS2OU5SBH1XQ</td>\n",
              "      <td>6304298285</td>\n",
              "      <td>Studebaker Hoch, billythemtn@geocities.com</td>\n",
              "      <td>Unbelieveably bad fare from the maker of such ...</td>\n",
              "      <td>Shame on you Francis</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   overall      reviewerID        asin  \\\n",
              "1        5  A16BJ43Z46QS3N  6304077955   \n",
              "2        4  A3AS6G2F9CDHV5  6304154178   \n",
              "3        2  A2JS2OU5SBH1XQ  6304233639   \n",
              "4        4  A2JS2OU5SBH1XQ  6304258984   \n",
              "5        1  A2JS2OU5SBH1XQ  6304298285   \n",
              "\n",
              "                                 reviewerName  \\\n",
              "1                                 C. L Wilson   \n",
              "2                                   Brian May   \n",
              "3  Studebaker Hoch, billythemtn@geocities.com   \n",
              "4  Studebaker Hoch, billythemtn@geocities.com   \n",
              "5  Studebaker Hoch, billythemtn@geocities.com   \n",
              "\n",
              "                                          reviewText  \\\n",
              "1  I didn't really look forward to seeing this mo...   \n",
              "2  This adventure, penned no less by Douglas Adam...   \n",
              "3  One can only guess at what the producers of th...   \n",
              "4  Sure, the plot is so full of holes that you co...   \n",
              "5  Unbelieveably bad fare from the maker of such ...   \n",
              "\n",
              "                                             summary  \n",
              "1                                           no title  \n",
              "2  \"They slammed him to the wall with good vibrat...  \n",
              "3                         Just what were they after?  \n",
              "4                              One Helluva Ride!!!!!  \n",
              "5                               Shame on you Francis  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f233Ue14ueG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Created test and train data in such a way, 80% of the users movie ratings are in train data and test 20% in test data\n",
        "overalldata = pd.concat([train_data,test_data])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "roIRao0VF_lS",
        "colab": {}
      },
      "source": [
        "top5_recommendations = getPredictions(train_data, moveidata, test_data, topN)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zGPqY9gCHEww",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}